---
layout: single
title:  "Differential Privacy, Correlated Data and Public Constraints"
categories: blog
---

<p>Some papers have highlighted a supposed weakness in the definition of <a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">differential privacy</a>, that it does not provide adequate privacy when the dataset has correlated rows or when some public constraints about the dataset are known.</p>
  
<p>Differential privacy is a mathematical notion of privacy for analysis of sensitive datasets, i.e., datasets composed of private data of individuals. One of the claimed benefits of differential privacy over other alternative definitions, e.g., k-anonymity, data de-anonymisation, is that the privacy guarantee holds even under arbitrary background knowledge of an adversary. The aforementioned weaknesses posit that this property of differential privacy is in fact not true in cases where the dataset has correlated rows or when some public constraints about the dataset are known.</p>

<p>The above criticism seems to be quite prevalent despite some rebuttals. However these arguments are not from a technical point of view, but rather the motivation behind a practical definition of privacy. Some remarks can also be found in technical papers which highlight that correlations are not considered a privacy violation. The aim of this blogpost is to discuss correlations in a semi-technical sense, and also highlight an intriguing issue with one example on "publicly known constraints" of the underlying dataset. We begin with what differential privacy promises, then discuss correlated data, and finally the issue of public constraints (a stronger form of correlation/background information).</p>

<h2>What is promised by differential privacy?</h2>

<p>TBC.</p>
