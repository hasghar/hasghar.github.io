---
layout: single
title:  "Differential Privacy and Publicly Known Constraints"
categories: blog
published: false
---

<p>Some <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">papers</a> <a href=https://arxiv.org/pdf/1312.3913.pdf>have</a> argued about a weakness in the definition of <a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">differential privacy</a>; namely that it does not provide privacy when the dataset has correlated rows or when some public constraints about the dataset are known.</p>
  
<p>Differential privacy is a mathematical notion of privacy for statistical analysis of sensitive datasets, i.e., datasets containing private data of individuals. One of the claimed benefits of differential privacy over other alternative definitions, e.g., <a href="https://dataprivacylab.org/dataprivacy/projects/kanonymity/paper3.pdf">k-anonymity</a>, data de-identification, is that the privacy guarantee holds even under <i>arbitrary background knowledge</i> of an adversary. The aforementioned weaknesses posit that this property of differential privacy is in fact not true in cases where the dataset has correlated rows or when some public constraints about the dataset are known. For instance, when exact query answers had already been released.</p>

<p>I will set aside the issue of correlated data, as others have already <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-29.md">critiqued</a> <a href="https://privacytools.seas.harvard.edu/files/privacytools/files/pdf_02.pdf">it</a>. Indeed, finding statistical correlations is the goal of data analysis in the first place. The focus of this blogpost is instead on releasing differentially private answers when <i>some constraints about the input dataset are already publicly known</i>. This example is rather intriguing, and seems like highlighting a weakness in the definition of differential privacy. In essence, this could be considered an extreme form of correlated data. I will reproduce the example first, which is detailed <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">here</a> and <a href=https://arxiv.org/pdf/1312.3913.pdf>here</a>.</p>

<h2>Publicly Known Constraints</h2>

<p> Suppose a company hires employees at exactly 'm' different salary levels: 1, 2, ..., m. At some point in time, the company decides to release the number of employees under each salary level, which we denote as n<sub>1</sub>, n<sub>2</sub>, ..., n<sub>m</sub>. The company wishes to do this in a privacy-preserving way, so that no one can tell the salary level of Alice, one of the employees of the company. We assume that Alice is at salary level 1. Let us further assume that there are 9 other employees with the same salary level as Alice. Thus, n<sub>1</sub> = 10. A standard way of releasing these counts via differential privacy is by adding <a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace noise</a> of scale 1/eps to each of the counts n<sub>1</sub>, n<sub>2</sub>, ..., n<sub>m</sub>, and then releasing the perturbed counts. Here "eps" stands for epsilon, the privacy budget. Let us denote the noisy counts by &ntilde<sub>1</sub>, &ntilde<sub>2</sub>, ..., &ntilde<sub>m</sub>.</p>

<p> Now, assume that the company had previously publicly released som constraints about this dataset (back when the company was less privacy conscious). More specifically, the company previously released the information:</p> 

<blockquote cite="https://www.huxley.net/bnw/four.html">
<p>The number of people at salary level 2 are 10 more than the number of people at salary level 1. That is: n<sub>2</sub> = n<sub>1</sub> + 10.</p>
</blockquote>

<p>Not only this, but the company also released similar information about other salary levels:</p>

<blockquote cite="https://www.huxley.net/bnw/four.html">
<p>n<sub>3</sub> = n<sub>1</sub> + a<sub>3</sub>, n<sub>4</sub> = n<sub>1</sub> + a<sub>4</sub>, ..., n<sub>m</sub> = n<sub>1</sub> + a<sub>m</sub>.</p>
</blockquote>

<p>Here a<sub>3</sub>, a<sub>4</sub>, ..., a<sub>m</sub> are positive integers, and we let a<sub>2</sub> = 10. With these constraints, we can write:</p>
<blockquote cite="https://www.huxley.net/bnw/four.html">
<p>n<sub>1</sub> = a<sub>2</sub> - n<sub>2</sub>, n<sub>1</sub> = a<sub>3</sub> - n<sub>3</sub>, n<sub>1</sub> = a<sub>4</sub> - n<sub>4</sub>, ..., n<sub>1</sub> = a<sub>m</sub> - n<sub>m</sub>.</p>
</blockquote>
These are a total of m - 1 constraints. Note that we still do not know what the n<sub>i</sub>'s are; there are m unknowns and m - 1 linear equations. However, once the noisy counts are released, we can replace n<sub>i</sub> with &ntilde<sub>i</sub> in the RHS of each of the constraints. Then, together with &ntilde<sub>1</sub>, this gives us m estimates of n<sub>1</sub>. We can average these m estimates, and then round the average to the nearest integer to get the true count n<sub>1</sub> with high probability (as a function of m). An attacker who knows all other employees' salary levels excluding Alice, can now know for sure that Alice has salary level 1. The success of this attack is shown below as a function of the number of attribute values (salary levels) m. The light blue band shows when the averaged out value is the true value of n<sub>1</sub>.</p>

<img src="https://hasghar.github.io/assets/images/dp-corr-ind-salary.png" alt="Ind Salary">

<p>The example seems to imply that differential privacy's guarantee of privacy against arbitrary background knowledge is overstated.</p>

<h2>The Catch</h2>

<p>There is, however, some misinterpretation. The differential privacy guarantee is more accurately stated as: the output of a differentially private computation will not harm you more if your data were to be included even against arbitrary background knowledge. What this means is that what can be inferred with Alice's data can also be inferred without Alice's data. I would like to stress that these guarantees are stated in an informal way. Slightly different way of saying it (but again informally) is that the two worlds, one in which Alice's data is used in the differentially private computation and one without Alice's data are not easily distinguishable. In the example above, if we take Alice out of the count, i.e., n<sub>1</sub> is now 9 instead of 10, then the noisy version of n<sub>1</sub> is also different, i.e., it is now 9 + Lap(1/eps). However, if we release this count instead of the noisy count with Alice, it is still possible to solve the above constraints to find out the original value of n<sub>1</sub>. In fact, even if we do not release n<sub>1</sub> at all, it is still possible to find the original value of n<sub>1</sub>. This is illustrated in the figure below.</p>  

<img src="https://hasghar.github.io/assets/images/dp-corr-ind-salary-wo.png" alt="Ind Salary WO">

<h2>So what's happening?</h2>

<p>The main issue here is that Alice's information is tied to other individuals' information via the previous non-private data release. Thus, even if the dataset without Alice's data was used in the differentially private computation, the noisy release still tells the adversary about Alice. We can think of an even simpler example. Suppose, due to some weird equity policy, the company ensures that whenever a new person is hired at a certain salary level, the company needs to hire m - 1 other individuals at the m - 1 other salary levels. Thus, now we have the constraints n<sub>i</sub> = n<sub>1</sub> for all i in {1, ..., m}. The data analyst, unaware of these constraints, releases n<sub>i</sub> + Lap(1/eps) for each i. Once again the output can be averaged out to find the true count.</p>

<p>Another way of interpreting differential privacy's guarantee is that the attacker needs to guess which of the two datasets is the input dataset given a differentially private output. One with n individuals (with Alice included) and the one with n - 1 individuals (without Alice). The adversary needs to guess whether the release was done with Alice's data or without Alice's data. We can see that the adversary will reach to the same conclusion in the two settings in the above example. The adversary cannot guess whether the data release was done with Alice's data or without. However, the constraints themselves were wieth Alice's data. The problem is that the constraints themselves are leaking way too much information that Alice is a sitting duck. Her data is tied to the data of at m - 1 other individuals. Her secret is no longer hers to keep.</p>

<p>It may be possible to release only one of the m attribute values using differential privacy, and then deduce the other m - 1 through the constraints. However, this requires knowing beforehand what is available out there. One of the advantages of differential privacy is not needing to be concerned about background or public information out there. The privacy guarantee is tied to an individual. If the individual's data is linked to every other individual's data (an extreme case of the m constraints) then there is an issue with the privacy expectation. The goal is to be able to infer database wide trends. Unfortunately, in this case database wide trends are directly you.</p> 
  
We can further assume that the attacker knows everything about the n - 1 individuals. However, with this and the publicly known constraints discussed above, the attacker already knows all it needs to know. That is, it can find n<sub>1</sub> from any of the m - 1 constraints, even without any differentially private release. So, we might want to assume that the constraints are different as well, i.e., the constraints are now n<sub>1</sub> = a<sub>2</sub> - n<sub>i</sub> + 1, for all i in {2, ..., m}. Now, the attacker cannot distinguish between the two cases. 

The above criticism seems to be quite prevalent despite some rebuttals. However these arguments are not from a technical point of view, but rather the motivation behind a practical definition of privacy. Some remarks can also be found in technical papers which highlight that correlations are not considered a privacy violation. The aim of this blogpost is to discuss correlations in a semi-technical sense, and also highlight an intriguing issue with one example on "publicly known constraints" of the underlying dataset. We begin with what differential privacy promises, then discuss correlated data, and finally the issue of public constraints (a stronger form of correlation/background information).</p>
