---
layout: single
title:  "Differential Privacy and Publicly Known Constraints"
categories: blog
published: false
---

<p>Some <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">papers</a> <a href=https://arxiv.org/pdf/1312.3913.pdf>have</a> argued about a weakness in the definition of <a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">differential privacy</a>, that it does not provide privacy when the dataset has correlated rows or when some public constraints about the dataset are known.</p>
  
<p>Differential privacy is a mathematical notion of privacy for analysis of sensitive datasets, i.e., datasets composed of private data of individuals. One of the claimed benefits of differential privacy over other alternative definitions, e.g., k-anonymity, data de-anonymisation, is that the privacy guarantee holds even under <i>arbitrary background knowledge</i> of an adversary. The aforementioned weaknesses posit that this property of differential privacy is in fact not true in cases where the dataset has correlated rows or when some public constraints about the dataset are known. For instance, when exact query answers had already been released.</p>

<p>I will set aside the issue of correlated data, as this has already been argued before. The focus of this blogpost is on the example of releasing <i>differentially private answers when some constraints about the input dataset are publicly known</i>. This example is rather intriguing, and seems like highlighting a weakness in the definition of differential privacy. In essence, this could be considered an extreme form of correlated data. I will reproduce the example first, which is detailed <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">here</a> and <a href=https://arxiv.org/pdf/1312.3913.pdf>here</a>.</p>

<h2>Publicly Known Constraints</h2>

<p> Suppose a company has 'm' different salary levels: 1, 2 upto m. The company decides to release the number of employees under each salary level, which we denote as n<sub>1</sub>, n<sub>2</sub>, ..., n<sub>m</sub>. The company wishes to do this in a privacy-preserving way, so that no one can tell the salary level of Alice, one of the employees of the company. We assume that Alice is at salary level 1. We assume that there are 9 other employees with the same salary level as Alice. Thus, n<sub>1</sub> = 10. A standard way of doing this is to add Laplace noise of scale 1/eps to each of the counts n<sub>1</sub>, n<sub>2</sub>, ..., n<sub>m</sub>, and release the noisy counts. Here "eps" stands for epsilon, the privacy budget. Let us denote the noisy counts by &ntilde<sub>1</sub>, &ntilde<sub>2</sub>, ..., &ntilde<sub>m</sub>.</p>

<p> Now, consider that due to previous releases (when the company was less fussy about privacy) some constraints about this dataset are known. More specifically, the company previously released the information: the number of people at salary level 2 are 10 more than the number of people at salary level 1. That is: n<sub>2</sub> = n<sub>1</sub> + 10. Not only this, but the company also released similar information about other salary levels: n<sub>3</sub> = n<sub>1</sub> + a<sub>3</sub>, n<sub>4</sub> = n<sub>1</sub> + a<sub>4</sub>, ..., n<sub>m</sub> = n<sub>1</sub> + a<sub>m</sub>. Here a<sub>3</sub>, a<sub>4</sub>, ..., a<sub>m</sub> are positive integers, and we let a<sub>2</sub> = 10.</p>

<p> With these constraints, we can write: n<sub>1</sub> = a<sub>2</sub> - n<sub>2</sub>, n<sub>1</sub> = a<sub>3</sub> - n<sub>3</sub>, n<sub>1</sub> = a<sub>4</sub> - n<sub>4</sub>, ..., n<sub>1</sub> = a<sub>m</sub> - n<sub>m</sub>. These are a total of m - 1 constraints. Note that we still do not know what the n<sub>i</sub>'s are. There are m unknowns and m - 1 linear equations. However, once the noisy counts are released, we can replace n<sub>i</sub> with &ntilde<sub>i</sub> in the RHS of each of the constraints. Then, together with &ntilde<sub>1</sub>, this gives us m estimates of n<sub>1</sub>. We can average these m estimates, and the round the average to the nearest integer to get the true count n<sub>1</sub> with high probability (as a function of m). An attacker who knows all other employees salary level excluding Alice, can now know for sure that Alice has salary level 1. The success of this attack is shown below as a function fo the number of attribute values (salary levels) m.</p>

<img src="https://hasghar.github.io/assets/images/dp-corr-ind-salary.png" alt="Ind Salary">

<p>The example seems to imply that differential privacy's guarantee of privacy against arbitrary background knowledge is overstated.</p>

<h2>The Catch</h2>

<p>There is, however, some misinterpretation. The differential privacy guarantee is more accurately stated as: the output of a differentially private computation will not harm you more if your data were to be included even against arbitrary background knowledge. What this means is that what can be inferred with Alice's data can also be inferred without Alice's data. I would like to stress that I am stating these guarantees in an informal way. Slightly more formal way of saying it is that the difference between the two worlds (with Alice's data and without Alice's data) would not be easily detectable. Anyway, we will stick to our informal interpretation. In the example above, if we take Alice out of the count, i.e., n<sub>1</sub> is now 9 instead of 10, then the noisy version of n<sub>1</sub> is now also different, i.e., it is now n<sub>1</sub> + Lap(1/eps). However, if we now release this count instead of the noisy count with Alice, it is still possible to solve the above constraints to find out the original value of n<sub>1</sub>. In fact, even if we do not release n<sub>1</sub> at all, it is still possible to find the original value of n<sub>1</sub>. This is illustrated in the figure below.</p>  

<img src="https://hasghar.github.io/assets/images/dp-corr-ind-salary-wo.png" alt="Ind Salary WO">

<p>So what's happening? The main issue here is that Alice's information is now tied to other individuals' information. Thus, even if the dataset without Alice's data was used, the noisy release still tells the adversary about Alice. We can think of an even simpler example. Suppose, due to some weird policy, the company ensures that whenever a new person is hired at a certain salary level, the company needs to hire m - 1 other individuals at the m - 1 other salary levels. Thus, we have the constraints n<sub>i</sub> = n<sub>1</sub> for all i in {1, ..., m}. The data analyst, unaware of these constraints, releases n<sub>i</sub> + Lap(1/eps) for each i. Once again the output can be averaged out to find the true count.</p>

<p>A better way of thinking about differential privacy's guarantee is that the attacker needs to guess which of the two datasets is the input dataset. The one with n (with Alice) and the one n - 1 (without Alice). The adversary needs to guess whether the release was done with Alice's data or without Alice's data. We can see that the adversary will reach to the same conclusion in the two settings in the above example. The adversary cannot guess whether the data release was done with Alice's data or without. However, the constraints themselves were wieth Alice's data. The problem is that the constraints themselves are leaking way too much information that Alice is a sitting duck. Her data is tied to the data of at m - 1 other individuals. Her secret is no longer hers to keep.</p>

<p>It may be possible to release only one of the m attribute values using differential privacy, and then deduce the other m - 1 through the constraints. However, this requires knowing beforehand what is available out there. One of the advantages of differential privacy is not needing to be concerned about background or public information out there. The privacy guarantee is tied to an individual. If the individual's data is linked to every other individual's data (an extreme case of the m constraints) then there is an issue with the privacy expectation. The goal is to be able to infer database wide trends. Unfortunately, in this case database wide trends are directly you.</p> 
  
We can further assume that the attacker knows everything about the n - 1 individuals. However, with this and the publicly known constraints discussed above, the attacker already knows all it needs to know. That is, it can find n<sub>1</sub> from any of the m - 1 constraints, even without any differentially private release. So, we might want to assume that the constraints are different as well, i.e., the constraints are now n<sub>1</sub> = a<sub>2</sub> - n<sub>i</sub> + 1, for all i in {2, ..., m}. Now, the attacker cannot distinguish between the two cases. 

The above criticism seems to be quite prevalent despite some rebuttals. However these arguments are not from a technical point of view, but rather the motivation behind a practical definition of privacy. Some remarks can also be found in technical papers which highlight that correlations are not considered a privacy violation. The aim of this blogpost is to discuss correlations in a semi-technical sense, and also highlight an intriguing issue with one example on "publicly known constraints" of the underlying dataset. We begin with what differential privacy promises, then discuss correlated data, and finally the issue of public constraints (a stronger form of correlation/background information).</p>
