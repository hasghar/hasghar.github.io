---
layout: single
title:  "Differential Privacy and Publicly Known Constraints"
categories: blog
published: false
---

<p>Some <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">papers</a> <a href=https://arxiv.org/pdf/1312.3913.pdf>have</a> argued about a weakness in the definition of <a href="https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf">differential privacy</a>, that it does not provide privacy when the dataset has correlated rows or when some public constraints about the dataset are known.</p>
  
<p>Differential privacy is a mathematical notion of privacy for analysis of sensitive datasets, i.e., datasets composed of private data of individuals. One of the claimed benefits of differential privacy over other alternative definitions, e.g., k-anonymity, data de-anonymisation, is that the privacy guarantee holds even under <i>arbitrary background knowledge</i> of an adversary. The aforementioned weaknesses posit that this property of differential privacy is in fact not true in cases where the dataset has correlated rows or when some public constraints about the dataset are known. For instance, when exact query answers had already been released.</p>

<p>I will set aside the issue of correlated data, as this has already been argued before. The focus of this blogpost is on the "exact query release" example. This example is rather intriguing, and does indeed seem like highlighting a weakness in differential privacy; unless, you dig a bit deeper. I will reproduce the example first, which is detailed <a href="http://www.cse.psu.edu/~duk17/papers/nflprivacy.pdf">here</a> and <a href=https://arxiv.org/pdf/1312.3913.pdf>here</a>.</p>

<h2>Exact Query Release</h2>


<p>The above criticism seems to be quite prevalent despite some rebuttals. However these arguments are not from a technical point of view, but rather the motivation behind a practical definition of privacy. Some remarks can also be found in technical papers which highlight that correlations are not considered a privacy violation. The aim of this blogpost is to discuss correlations in a semi-technical sense, and also highlight an intriguing issue with one example on "publicly known constraints" of the underlying dataset. We begin with what differential privacy promises, then discuss correlated data, and finally the issue of public constraints (a stronger form of correlation/background information).</p>

<h2>What is promised by differential privacy?</h2>

<p>TBC.</p>
